<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
	<title>24 Perception</title>
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css"
		integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">
	<link rel="stylesheet" href="../styles.css">
	<link rel="stylesheet" href="./css/style.css">
	<script src="https://code.jquery.com/jquery-1.12.4.js"></script>
	<script type="text/javascript" src="../main.js"></script>
</head>

<body>

	<div class="container-fluid">
		<div class="row">
			<div class="col-sm-6 col-md-offset-3" id="content">

				<h1>Perception: Edge Detection</h1>
				<br>

				<h2>Introduction</h2>
				<p>
					For many of us, it is easy to find both the objects in a picture and their boundaries.
					Humans are very good at identifying what something is and where it is given a clear line of
					sight, but what if we need to find the boundaries of objects in an enormously large number photos?
					While we could put a room full of toddlers with markers at work and tell them to outline the edges
					in all the photos, we can do better in both performance and morals by making a computer do this task
					for us! In fact, in computer vision, this is a general problem known as <strong>edge detection</strong>.
				</p>
				<p>
					Formally, edge detection is the process of detecting the boundaries in an image that separate great
					changes in brightness or <strong>intensity</strong>. One of the reasons to do edge detection is that it is
					able to reduce a complex image down to a simpler one where only these edges are marked. Because of this,
					edge detection often acts as a preliminary step to other computer vision processes.
				</p>

				<br>
				<img src="../third-party/photo-edge-detection.png" class="center display-image">
				<br><br>

				<h2>What is an Image?</h2>
				<p>
					Before we dive into edge detection, we must talk about how an image is represented on a machine.
					While our toddler workforce may work with physical photographs, computers work with images as 2D
					grids known as <strong>pixel arrays</strong>. A color image, for instance, is commonly represented with 3 grids,
					one for each of the red, green, and blue color channels. With edge detection, since we are generally
					only interested in the intensity of a pixel, or how bright or dark it is, we will often first convert
					our color images to <strong>grayscale</strong> intensities which are only made up of one grid that stores the intensity
					of each pixel.
				</p>
				<br>

				<br>
				<!--div id="pipeline-grayscale-root"></div-->
				<br>

				<h2>Detecting Edges</h2>
				<p>
					How do we go about solving the problem of detecting edges? Since our goal is to find locations where image
					intensity changes the most, we need some way of finding out how much the intensity changes as we move across
					the pixels in our image.
				</p>
				<p>
					For problems like these, it helps to add an extra dimension for visualization. Let us re-imagine our pixel
					array as a 3D topological map, representing the intensity of each pixel as some height. Below is an example
					of how one of these maps might look. Drag it around and observe how pixel intensities are translated into 3D
					elevation.
				</p>

				<br>
				<div id="topology-root"></div>
				<br>

				<p>
					Picking some arbitrary point on this map, we now want to figure out the change in elevation at this location.
					Knowing the elevation at the chosen point alone is relatively unhelpful since, although it may tell us how high
					up it is, we get no information about how the elevation changes around our point. What if, in addition, we also
					considered the elevation of land a small distance to the west and east of our location? Ah-hah! Now by comparing
					those elevations, we can evaluate a very crude metric of the change in elevation moving eastward around our point!
				</p>

				<br>
				<img src="./images/elevation.png" class="center display-image">
				<br><br>

				<p>
					This is the driving principle behind a well-known edge detector called the Sobel operator. The operator works
					by dragging two <strong>kernels</strong> or <strong>filters</strong> called Sobel X and Sobel Y across an
					image to calculate horizontal and vertical change respectively. For each target pixel, the kernel covers and
					operates on a neighborhood of surrounding pixels from which we use to calculate the change of intensity at the
					target. To demonstrate this, below is an interactive example using the Sobel X filter. Brighter areas in the
					result are indicative of large positive change while darker areas are indicative of large negative change.
					Use your mouse to drag the filter around and see how different areas reflect different intensity changes.
				</p>

				<br>
				<div id="convolution-root"></div>
				<br>

				<p>
					Numerically, these filters are actually grids of values that are matched up to a neighborhood of
					pixels and used in a weighted sum to calculate the change at each pixel. In general, this concept of
					operating on a grid of values by dragging a smaller grid across it is a powerful mathematical idea that
					goes beyond edge detection known as <strong>convolution</strong>.
				</p>

				<br>
				<img src="./images/filters.png" class="center display-image">
				<br><br>

				<p>
					The end result of applying both the Sobel X and Sobel Y filters to our image is two grids containing
					values that represent horizontal and vertical change respectively. These values can also be thought of
					as the X and Y components for a 2D arrow or <strong>vector</strong>. The vector gotten using the Sobel
					operator is known as a gradient whose magnitude represents how much change occurs at its location and
					whose angle represents the direction of the greatest change.
				</p>

				<p>
					Below is a visualization of an imageâ€™s gradient. Draw edges or select one of the preset images and
					observe how the direction and magnitude of the gradient vectors change in response. The magnitude of
					the vectors is indicated by color with blue being small and red being large.
				</p>

				<br>
				<div id="gradient-root"></div>
				<br>

				<p>
					One can see why calculating gradients for intensity in our photograph using Sobel is a start to finding edges.
					Locations where the gradient magnitudes are small mean little intensity changes and no edges while locations
					where the gradient magnitudes are large mean big intensity changes and potential edges.
				</p>

				<br>
				<!--div id="pipeline-grad-root"></div-->
				<br>

				<h2>Gaussian Smoothing</h2>
				<p>
					The Sobel operator is a good start at edge detection, but it comes with its own share of faults.
					One problem with solely using Sobel is that it is very susceptible to <strong>noise</strong>. Our current approach
					would count a single bright pixel in a sea of dark pixels as an edge which is undesirable.
				</p>

				<br>
				<img src="../third-party/shannon-01.png" class="center display-image">
				<br><br>

				<p>
					To reduce noise in an image, we can choose to do <strong>Gaussian smoothing</strong> before attempting to apply
					the Sobel operator. How Gaussian smoothing is done is very similar to Sobel; using a filter and
					convolution to change the source image. The filter this time, however, is an approximation of the
					Gaussian function or normal distribution in 2D.
				</p>

				<br>
				<img src="../third-party/gaussian-2d.png" class="center display-image">
				<br><br>

				<p>
					Applying this filter has the effect of smoothing out the intensities in an image which then
					allows an edge detector, like Sobel, to produce better results.
				</p>

				<br>
				<img src="../third-party/shannon-02.png" class="center display-image">
				<br><br>

				<h2>Non-Maximum Suppression</h2>
				TODO

				<br>
				<div id="suppression-root"></div>
				<br>

				<br>

				<h2>Double Threshold</h2>
				TODO
				<br>

				<h2>Edge Tracking by Hysteresis</h2>
				TODO
				<br>

				<h2>Canny Edge Detection</h2>
				TODO

				<br>
				<div id="sobel-image-root"></div>
				<br>

				<br>
				<br>

			</div>
		</div>
	</div>


	<!-- Replace with third party -->
	<script crossorigin src="https://unpkg.com/react@16/umd/react.development.js"></script>
	<script crossorigin src="https://unpkg.com/react-dom@16/umd/react-dom.development.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.min.js"></script>

	<script type="text/javascript" src="./js/setup.js"></script>
	<script type="text/javascript" src="./js/util.js"></script>
	<script type="text/javascript" src="./js/ui.js"></script>
	<script type="text/javascript" src="./js/imageProcessing.js"></script>

	<script type="text/javascript" src="./js/demoConvolution.js"></script>
	<script type="text/javascript" src="./js/demoGradient.js"></script>
	<script type="text/javascript" src="./js/demoSobelImage.js"></script>
	<script type="text/javascript" src="./js/demoTopology.js"></script>
	<script type="text/javascript" src="./js/demoSuppression.js"></script>
	<script type="text/javascript" src="./js/demoPipeline.js"></script>

	<script type="text/javascript">
		// mobile setup
		document.ontouchmove = function (event) {
			event.preventDefault();
		}
	</script>

</body>

</html>