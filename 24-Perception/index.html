<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
	<title>24 Perception</title>
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css"
		integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">
	<link rel="stylesheet" href="../styles.css">
	<link rel="stylesheet" href="./css/style.css">
	<script src="https://code.jquery.com/jquery-1.12.4.js"></script>
	<script type="text/javascript" src="../main.js"></script>
</head>

<body>

	<div class="container-fluid">
		<div class="row">
			<div class="col-sm-6 col-md-offset-3" id="content">

				<h1>Perception: Edge Detection</h1>
				<br>

				<h2>Introduction</h2>
				<p>
					For many of us, it is easy to find both the objects in a picture and their boundaries.
					Humans are very good at identifying what something is and where it is given a clear line of
					sight, but what if we need to find the boundaries of objects in an enormously large number photos?
					While we could put a room full of toddlers with markers at work and tell them to outline the edges
					in all the photos, we can do better in both performance and morals by making a computer do this task
					for us! In fact, in computer vision, this is a general problem known as <strong>edge detection</strong>.
				</p>
				<p>
					Formally, edge detection is the process of detecting the boundaries in an image that separate great
					changes in brightness or <strong>intensity</strong>. One of the reasons to do edge detection is that it is
					able to reduce a complex image down to a simpler one where only these edges are marked. Because of this,
					edge detection often acts as a preliminary step to other computer vision processes.
				</p>

				<br>
				<img src="../third-party/photo-edge-detection.png" class="center">
				<br><br>

				<h2>What is an Image?</h2>
				<p>
					Before we dive into edge detection, we must talk about how an image is represented on a machine.
					While our toddler workforce may work with physical photographs, computers work with images as 2D
					grids known as <strong>pixel arrays</strong>. A color image, for instance, is commonly represented with 3 grids,
					one for each of the red, green, and blue color channels. With edge detection, since we are generally
					only interested in the intensity of a pixel, or how bright or dark it is, we will often first convert
					our color images to <strong>grayscale</strong> which are only made up of one grid that stores the intensity
					of each pixel.
				</p>
				<br>

				<br>
				<div id="pipeline-grayscale-root"></div>
				<br>

				<h2>Detecting Edges</h2>
				<p>
					How do we go about solving the problem of detecting edges? Since our goal is to find locations where image
					intensity changes the most, we need some way of finding out how much the intensity changes as we move across
					the pixels in our image.
				</p>
				<p>
					For problems like these, it helps to add an extra dimension for visualization. Let us instead
					represent our intensity pixel array as a 3D topological map.	
				</p>

				<br>
				<div id="topology-root"></div>
				<br>

				<p>
					Picking some arbitrary point on this terrain, and we want to figure out whether it is along the
					boundary of a cliff or ravine. Knowing the elevation at our chosen point alone is relatively unhelpful
					since, although it may tell us how high up it is, we get no information about how the elevation changes
					around our point. What if, in addition, we also considered the elevation of land a small distance to the
					west and east of our point? Ah-hah! Now by taking the difference between those elevations, we can evaluate
					a very crude metric of the change in elevation moving eastward at that location!
				</p>

				<br>
				<img src="./images/elevationPlaceholder.png" class="center">
				<br><br>

				<p>
					This is the driving principle behind a well-known edge detector called the <strong>Sobel operator</strong> which uses
					information about change across pixels to find edges. Sobel consists of dragging two grids known as <strong>kernels</strong>
					or <strong>filters</strong> across an image to independently calculate intensity changes for each pixel in the horizontal and
					vertical directions by considering the differences in intensity among neighboring pixels.
				</p>

				<br>
				<img src="./images/filters.png" class="center">
				<br><br>

				<p>
					When applied, the filters are flipped both horizontally and vertically, and the values in the filter are used in a
					weighted sum along with the intensity values of the pixels in the area it covers to calculate change in intensity.
					Below is a demonstration of the Sobel X filter. Drag your mouse or use the control panel to see how the filter calculates
					change when applied to different parts of the source image. You can change the values in the filter and source and see how
					it affects the change calculation as well.
				</p>

				<br>
				<div id="convolution-root"></div>
				<br>

				<p>
					As a brief aside, this concept of operating on a grid of values by dragging a smaller grid across it is a powerful
					and general idea that goes beyond edge detection. When we performed the Sobel operation, note that we flipped the
					kernel horizontally and vertically before applying it, doing what is generally known a <strong>convolution</strong>. By applying the
					kernel directly without modification, we do a similar but different operation known as a <strong>cross correlation</strong>.
					Both yield similar results, but in practice, convolution is preferred over cross correlation because convolutions
					generally have nicer mathematical properties.
				</p>

				<p>
					The end result of applying both the Sobel X and Sobel Y filters is two grids containing values that represent horizontal
					and vertical change respectively. These values can also be thought of as the X and Y components for a 2D intensity arrow
					or <strong>vector</strong> at each pixel. The vector gotten using the Sobel operator is known as a <strong>gradient</strong>
					whose magnitude represents how much intensity change occurs at its location and whose angle represents the
					direction of the greatest change.
				</p>

				<p>
					Below is a visualization of an imageâ€™s gradient. Change the values in the array and observe how the direction and magnitude
					of the gradient vectors change in response. The magnitude of the vectors is indicated by color with blue being very small
					and red being very large.
				</p>

				<br>
				<div id="gradient-root"></div>
				<br>

				<p>
					One can see how calculating gradients for intensity in our photograph using Sobel is a start to finding edges.
					Locations where the gradient magnitudes are small mean little intensity changes and no edges while locations
					where the gradient magnitudes are large mean big intensity changes and potential edges. To recap, below is a
					demo of this edge detection pipeline so far. Change the selected pixel with your mouse or the control panel.
				</p>

				<br>
				<div id="pipeline-grad-root"></div>
				<br>

				<h2>Gaussian Smoothing</h2>
				TODO

				<h2>Non-Maximum Suppression</h2>
				TODO

				<br>
				<div id="suppression-root"></div>
				<br>

				<br>

				<h2>Double Threshold</h2>
				TODO
				<br>

				<h2>Edge Tracking by Hysteresis</h2>
				TODO
				<br>

				<h2>Canny Edge Detection</h2>
				TODO

				<br>
				<div id="sobel-image-root"></div>
				<br>

				<br>
				<br>

			</div>
		</div>
	</div>


	<!-- Replace with third party -->
	<script crossorigin src="https://unpkg.com/react@16/umd/react.development.js"></script>
	<script crossorigin src="https://unpkg.com/react-dom@16/umd/react-dom.development.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.min.js"></script>
	<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

	<script type="text/javascript" src="./js/util.js"></script>
	<script type="text/javascript" src="./js/ui.js"></script>
	<script type="text/javascript" src="./js/imageProcessing.js"></script>

	<script type="text/javascript" src="./js/demoConvolution.js"></script>
	<script type="text/javascript" src="./js/demoGradient.js"></script>
	<script type="text/javascript" src="./js/demoSobelImage.js"></script>
	<script type="text/javascript" src="./js/demoTopology.js"></script>
	<script type="text/javascript" src="./js/demoSuppression.js"></script>
	<script type="text/javascript" src="./js/demoPipeline.js"></script>

	<script type="text/javascript">
		// mobile setup
		document.ontouchmove = function (event) {
			event.preventDefault();
		}
	</script>

</body>

</html>